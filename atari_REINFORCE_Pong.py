import numpy as np
import torch
import gym
from atari_py import import_roms
import torch.nn as nn
from torch.optim import RMSprop
import torch.nn.functional as F
from tqdm import tqdm as tq
from torch.distributions import Categorical
from torch.utils.data import Dataset,DataLoader
from torch.optim import Adam,AdamW
from neural_nets import pi_net
from atari_env_wrapper import atari_env_wrapper






        
            
        

            
class atari_VPG():
    """ The Class implements Vanilla Policy Gradient algorithm for atari games.Experiance is generated by a ConvNet Policy,defined by the nn Module
        pi_net.baseline b for normlizing monte carlo returns (discounted sum of rewards till end of the episode) is taken as 0.An episode of experiance 
        is generated and then a policy gradient in estimated (gradient of expected sum of rewards starting in the init state distribution and ending with episode
        wrt. the policy parameters.
            """
    def __init__(self,pi_net_arch,device=torch.device('cpu'),batch_size=32,kframes=2,
                 env_wrapper=atari_env_wrapper,gamma=0.99,policy_loss=nn.CrossEntropyLoss(reduction='none')):
       
        self.batch_size=batch_size
        self.env_wrapper=env_wrapper(env=gym.make('Pong-v0'),kframes=kframes)
         ## selecting only the up and down actions as valid
        self.valid_actions=[2,3]
        self.n_actions=len(self.valid_actions)
        self.device=torch.device('cuda') if torch.cuda.is_available() else device
        print(f'using device:{self.device}')
        self.pi_net=pi_net_arch(n_actions=self.n_actions,kframes=kframes).to(device)
        self.gamma=gamma
        ## selecting only the up and down actions as valid
        
        self.policy_loss=policy_loss.to(self.device)
        
    ## torch.utils.dataset wrapped around trajectory input frames and targets,to create Dataloaders from
    class trajectory_dataset(Dataset):
        def __init__(self,trajectory):
            self.trajectory=trajectory
        def __len__(self):
            return len(self.trajectory)
        def __getitem__(self,idx):
            return self.trajectory[idx]
            
            
        
    def pi_forward(self,proc_frames_batch):
        return self.pi_net(proc_frames_batch.to(self.device))
        
        
    def select_action(self,proc_frames_batch):
        ## act wrt the current policy (pi_net)
        with torch.no_grad():
            ## policy net takes batch of input states to prescribe actions,
            ## hence for a single state have to pre append a leading dim 
            probs = F.softmax(self.pi_forward(proc_frames_batch.unsqueeze(0)),dim=0)
        # sample action from current policy i.e pi a|s
        m = Categorical(probs)
        action_index=m.sample()
        action =self.valid_actions[action_index]
        return action,action_index
    
    def sample_rollout(self):
        proc_frames_batch,rew,done=self.env_wrapper.reset()
        ep_act_inds,ep_rews,ep_frames,d_factors=[],[],[],[]
        d_factor,total_epi_rew=1,0
        while not done:
            action,action_index=self.select_action(proc_frames_batch)
            proc_frames,rew,done=self.env_wrapper.step(action)
            ep_frames.append(proc_frames)
            ep_act_inds.append(action_index)
            ep_rews.append(rew*d_factor)
            d_factors.append(d_factor)
            d_factor*=self.gamma
            total_epi_rew+=rew
        
        ep_rews,d_factors=(torch.tensor(x) for x in [ep_rews,d_factors])
        ## reverse because the expected sum of future reward is the sum of discounted reward from each timestep onward
        pi_targets=ep_rews.sum()-torch.cumsum(ep_rews,dim=0)
        ## rescale takgets by removing extra gamma powers (discount factor terms)- not because sutton and bartp has extra gamma**t term for each time t
        ## apart from return 
        ##pi_targets=pi_targets/d_factors
        return list(zip(ep_frames,ep_act_inds,pi_targets)),total_epi_rew
            
        
        
    def train(self,n_episodes,optimizer_cls=RMSprop,lr=0.001):
        optimizer=optimizer_cls(self.pi_net.parameters(),lr)
        for n in tq(range(n_episodes),position=0,leave=True):
            trajectory,total_rew=self.sample_rollout()
            traj_ds=self.trajectory_dataset(trajectory)
            traj_dl=DataLoader(dataset=traj_ds,batch_size=32)
            episode_loss=[]
            for proc_frames_batch,act_inds,pi_targets in traj_dl:
                out=self.pi_forward(proc_frames_batch)
                try:
                    policy_loss=(self.policy_loss(out,act_inds.to(self.device))*pi_targets.to(self.device)).mean()
                ## to cover the case when dataloader returns single item-frames and actions
                except:
                    policy_loss=-(self.policy_loss(out.unsqueeze(0),act_inds.to(self.device))*pi_targets.to(self.device)).mean()
                optimizer.zero_grad()
                policy_loss.backward()
                optimizer.step()
                episode_loss.append(policy_loss)
            if n%10==0:
                print({'episode':n,'episode_reward':total_rew,'episode_length':len(trajectory),'episode_loss':torch.stack(episode_loss).detach().mean().cpu().item()})
            
          
           
                      




if __name__=='__main__' :
    VPG=atari_VPG(pi_net_arch=pi_net,gamma=0.99)
    VPG.train(50,lr=0.001,optimizer_cls=Adam)
   
                    
                    

            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
                    
                    
                    
                
                
                
                
                
                
            
            
            
            
        
        
        
        
        
        
        
        
        
        
        
    
        