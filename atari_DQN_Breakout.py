import numpy as np
import torch
from atari_py import import_roms
import torch.nn as nn
import copy
from torch.optim import RMSprop
import gym
import torchvision.transforms.functional as TF
from tqdm import tqdm as tq
import random
import os
from gym.core import Env
from atari_env_wrapper import atari_env_wrapper
from neural_nets import Q_net




        
        
        

            
                    

class atari_DQN():
    """ The Class implements DQN algorithm for atari games.Experiance is generated by acting epsilon greedily wrt the currently stored Q (action value) function
         policy evaluation is done by minimizing bellman error from generated experiance,policy improvement is done by epsilon greedy sampling of actions.
            """
    def __init__(self,Q_net_arch,kframes,device=torch.device('cpu'),training_begins=10000,buffer_size=100000,update_tgt_lag=1000,batch_size=32,
                 env_wrapper=atari_env_wrapper,gamma=0.99,bellman_loss=nn.MSELoss(),eps_start=1.0,eps_end=0.1):
       
        self.batch_size=batch_size
        self.buffer=self.replay_buffer(buffer_size)
        self.env_wrapper=env_wrapper(env=gym.make('Breakout-v0'),kframes=kframes)
        self.device=torch.device('cuda') if  torch.cuda.is_available() else device
        print(f'using device:{self.device}')
        self.Q_net=Q_net_arch(kframes=kframes).to(device)
        self.Q_targ=copy.deepcopy(self.Q_net)
        self.eps_start=eps_start
        self.eps_end=eps_end
        self.gamma=gamma
        self.bellman_loss=bellman_loss.to(self.device)
        self.update_tgt_lag=update_tgt_lag
        self.training_begins=training_begins
        self.rew_per_epi_log='reward_per_episode_log.csv'
    
    class replay_buffer():
        def __init__(self,max_size):
            self.max_size=max_size
            self.items=[]
            
        def add_item(self,item):
            self.items.append(item)
            if len(self.items)>self.max_size:
                self.items.pop(0)
                
        def sample_items(self,batch_size):
            return random.sample(self.items,min(batch_size,len(self.items)))
            
            
    def Q_net_forward(self,proc_frames_batch):
        return self.Q_net(proc_frames_batch.to(self.device))
    
    def Q_targ_forward(self,proc_frames_batch):
        return self.Q_targ(proc_frames_batch.to(self.device))
        
        
    def select_action(self,proc_frames,eps):
        ## act epsilon greedily wrt current Q function
        if np.random.rand()<=eps:
            return self.env_wrapper.action_space.sample()
        else:
            with torch.no_grad():
                   return torch.argmax(self.Q_net_forward(proc_frames.unsqueeze(0)))
                
  
        
    def eval_agent(self,eps=0.05,epis=100):
        all_rews=[]
        for ep in range(epis):
            _,epi_rew,done=self.env_wrapper.reset()
            while not done:
                action=self.select_action(frames,eps)
                _,rew,done=self.env_wrapper.step(action)
                epi_rew+=rew
            all_rews.append(epi_rew)
        all_rews=np.array(all_rews)
        
        return {'mean_rew_per_epi':all_rews.mean(),
                'std_dev_rew':all_rews.std()} 
                
                
    def train(self,timesteps,optimizer_cls=RMSprop,lr=0.001):
        ## set up decay factor for random exploration-epsilon
        eps_anneal_factor=np.exp(np.log(self.eps_end/self.eps_start)/(-self.training_begins+timesteps))
        optimizer=optimizer_cls(self.Q_net.parameters(),lr)
        eps=self.eps_start
        print(eps_anneal_factor)
        epis,total_rew,episode_rew,epoch_t_minus1=0,0,0,0
        proc_frames,rew,done=self.env_wrapper.reset()
       
        for t in tq(range(timesteps),position=0,leave=True):
            proc_frames_t=proc_frames
            a=self.select_action(proc_frames_t,eps)
           
            proc_frames,rew,done=self.env_wrapper.step(a)
            total_rew+=rew
            episode_rew+=rew
            
            if done:
                with torch.no_grad():
                    target=torch.tensor(rew).to(self.device)
                    proc_frames,rew,done=self.env_wrapper.reset()
                epis+=1
                if epis%10==0:
                    row={'episode':epis,'reward_per_episode':total_rew/epis,'reward_this_episode':episode_rew,'epsilon':eps}
                    print(row)
                    #df=pd.DataFrame(row)
                    #df.to_csv(self.rew_per_epi_log, mode='a', header=not os.path.exists(self.rew_per_epi_log))
                episode_rew=0    
                
            else:
                proc_frames_t_plus1=proc_frames
                a_prime=self.select_action(proc_frames_t_plus1,eps)
                with torch.no_grad():
                    ## need an extra leading dim to do forwarf pass for 1 sample minibatch
                    target=rew+self.gamma*self.Q_targ_forward(proc_frames_t_plus1.unsqueeze(0))[a_prime]
                
            ## add the tuple of Q_input and taget to replay buffer    
            self.buffer.add_item((proc_frames_t,a,target))
            if t>self.training_begins:
                train_steps=t-self.training_begins
                if train_steps==0:
                    print('training_begins')
                batch=self.buffer.sample_items(self.batch_size)
                x_b,a_b,y_b=zip(*batch)
                #calculating q(x,a) for all actions
                out_b=self.Q_net_forward(torch.stack(x_b))
                #calculating q(x,a) for the action selected
                out_b=torch.stack([out[a] for out,a in zip(out_b,a_b)])
                #calculating bellman error for minibatch and do backward step
                bellman_loss=self.bellman_loss(out_b,torch.stack(y_b))
                optimizer.zero_grad()
                bellman_loss.backward()
                optimizer.step()
                
                ## decay epsilon exploration
                eps=eps*eps_anneal_factor
                
                ## printing loss statistics every n=50 epochs
                epoch_t=(train_steps*self.batch_size)//self.buffer.max_size
                if epoch_t-epoch_t_minus1==50:
                    print ({'epoch':epoch_t,'bellman_loss':bellman_loss.detach().item()})
                epoch_t_minus1=epoch_t
                
                ## update the target Q network with a lagged copy of the Q  network
                if t%self.update_tgt_lag==0:
                    self.Q_targ=copy.deepcopy(self.Q_net)
                    
                    
if __name__=='__main__' :
    dqn=atari_DQN(Q_net_arch=Q_net,kframes=4,training_begins=500,buffer_size=1000,update_tgt_lag=1000,eps_start=0.3,eps_end=0.05)
    dqn.train(timesteps=10000,lr=0.001)
    
    

            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
                    
                    
                    
                
                
                
                
                
                
            
            
            
            
        
        
        
        
        
        
        
        
        
        
    
        